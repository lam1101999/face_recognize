{"cells":[{"cell_type":"markdown","metadata":{"id":"6xKnO_NXt3fq"},"source":["#Install libary\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9302,"status":"ok","timestamp":1665329786013,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"TtXBp_Qwt74j","outputId":"cdb0dd37-d927-446b-a025-b41b7fce6d5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.18.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n","\u001b[K     |████████████████████████████████| 1.9 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (2.23.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet-pytorch) (0.13.1+cu113)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet-pytorch) (2022.9.24)\n","Requirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (1.12.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet-pytorch) (4.1.1)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.2\n"]}],"source":["# !pip install black[jupyter]\n","!pip install -U tensorflow-addons\n","!pip install facenet-pytorch"]},{"cell_type":"markdown","metadata":{"id":"H6fok1Kvq1oj"},"source":["#Import and Initial Mount Disk"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32908,"status":"ok","timestamp":1665329818910,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"},"user_tz":-420},"id":"2CpvcU6MnbV6","outputId":"023fdcf6-8c79-4d4a-c330-543342b3e6a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import os\n","# Mount drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","path = \"/content/drive/My Drive/Colab Notebooks/FaceMaskRecognize\"\n","os.chdir(path)\n","\n","import cv2\n","import io\n","import pickle\n","import math\n","from scipy.spatial.distance import cosine\n","from sklearn.preprocessing import normalize\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import tensorflow_datasets as tfds\n","import matplotlib.pyplot as plt\n","import csv\n","from PIL import Image\n","import torch\n","import torchvision.transforms as transforms\n","from train_tensorflow.Net import InceptionResNetV1\n","from train_tensorflow.FaceNet import FaceNetModel,call_instance_FaceNet_with_custom, call_instance_FaceNet_without_custom, call_instance_FaceNet_with_last_isDense,convert_train_model_to_embedding\n","from train_tensorflow.Classify import Classify\n","from tool.FormatFunction import FormatFunction\n","from tool.FileFunction import FileFunction\n","from tool.GlobalValue import GlobalValue"]},{"cell_type":"markdown","metadata":{"id":"TYCbQnepU7aG"},"source":["#Format Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jlOuVvEvU-Up"},"outputs":[],"source":["# !black TensorFlow.ipynb"]},{"cell_type":"markdown","metadata":{"id":"u7Bkg28MAx3-"},"source":["# Build train model"]},{"cell_type":"markdown","metadata":{"id":"h7xVYDnAktTn"},"source":["## Init value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r_4KeVFgkvqc"},"outputs":[],"source":["# global_value = GlobalValue(image_size=[80,80], batch_size = 48, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","# format_function = FormatFunction(global_value)"]},{"cell_type":"markdown","metadata":{"id":"X6gRe_BuA1s6"},"source":["## Test Train Model tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szd-zIjl_G2w"},"outputs":[],"source":["# train_dataset, test_dataset = tfds.load(name=\"mnist\", split=['train', 'test'], as_supervised=True)\n","\n","# # Build your input pipelines\n","# train_dataset = train_dataset.map(format_function.format_gray_image)\n","# train_dataset = train_dataset.shuffle(1024).batch(global_value.BATCH_SIZE)\n","# test_dataset = test_dataset.map(format_function.format_gray_image)\n","# test_dataset = test_dataset.batch(global_value.BATCH_SIZE)\n","\n","# # The embedding model\n","# input_shape = [global_value.IMAGE_SIZE[0], global_value.IMAGE_SIZE[1], 3]\n","# face_net_model = call_instance_FaceNet_with_last_isDense(input_shape,10)\n","\n","\n","# # Compile the model\n","# face_net_model.compile(\n","#     optimizer=tf.keras.optimizers.Adam(0.001),\n","#     loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n","#     )\n","\n","\n","# # Train the network\n","# history = face_net_model.fit(\n","#       train_dataset,\n","#       epochs=global_value.EPOCHS\n","#       )\n","# face_net_model.save(\"save_model/modify_train_mnist\")\n","\n","\n","# # Evaluate the network\n","# results = face_net_model.predict(test_dataset)\n","\n","# # Save test embeddings for visualization in projector\n","# np.savetxt(\"vecs_48.tsv\", results, delimiter='\\t')\n","\n","# out_m = io.open('meta_48.tsv', 'w', encoding='utf-8')\n","# for img, labels in tfds.as_numpy(test_dataset):\n","#     [out_m.write(str(x) + \"\\n\") for x in labels]\n","# out_m.close()\n","\n","\n","# try:\n","#   from google.colab import files\n","#   files.download('vecs_48.tsv')\n","#   files.download('meta_48.tsv')\n","# except:\n","#   pass"]},{"cell_type":"markdown","metadata":{"id":"3ZxjStRYA47o"},"source":["## Save and Load model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nbCuEXp79xsa"},"outputs":[],"source":["# # save model\n","# face_net_model.save(\"save_model/number_mnist_48\")\n","# # save loss value\n","# loss_array = history.history['loss']\n","# with open(\"src/loss/mnist_loss.pkl\", 'wb') as file:\n","#     pickle.dump(loss_array, file)\n","\n","# reconstruct_model = tf.keras.models.load_model(\"save_model/modify_train_mnist\")\n","# reconstruct_model = convert_train_model_to_embedding(reconstruct_model)\n","# results = reconstruct_model.predict(test_dataset)\n","# # Save test embeddings for visualization in projector\n","# np.savetxt(\"vecs_48.tsv\", results, delimiter='\\t')\n","\n","# out_m = io.open('meta_48.tsv', 'w', encoding='utf-8')\n","# for img, labels in tfds.as_numpy(test_dataset):\n","#     [out_m.write(str(x) + \"\\n\") for x in labels]\n","# out_m.close()\n","\n","\n","# try:\n","#   from google.colab import files\n","#   files.download('vecs_48.tsv')\n","#   files.download('meta_48.tsv')\n","# except:\n","#   pass"]},{"cell_type":"markdown","metadata":{"id":"ZS12L0cD9xsa"},"source":["## Encoding Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdEkRQ_J9xsa"},"outputs":[],"source":["# train_dataset, test_dataset = tfds.load(name=\"mnist\", split=['train', 'test'], as_supervised=True)\n","# encoding_dic  = dict()\n","\n","# for i in range (10):\n","#     data_set = train_dataset.filter(lambda image,label: tf.equal(label,i))\n","#     data_set = data_set.map(format_function.format_gray_image)\n","#     data_set = data_set.batch(10)  \n","#     encodes = reconstruct_model.predict(data_set)\n","#     if encodes.any():\n","#         encodes = np.average(encodes, axis=0 )\n","#         encoding_dic[i] = encodes\n","\n","# with open(\"encodings/encoding_train_cross_entropy.pkl\", 'wb') as file:\n","#     pickle.dump(encoding_dic, file)"]},{"cell_type":"markdown","metadata":{"id":"R0dbgCnM9xsb"},"source":["## Predict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cR3Z8ggmHvvp"},"outputs":[],"source":["# def load_pickle(path):\n","#     with open(path, 'rb') as f:\n","#         encoding_dict = pickle.load(f)\n","#     return encoding_dict\n","\n","# recognition_t=5\n","\n","# def detect(image, encoder,encoding_dict):\n","#     image = tf.expand_dims(image,0)\n","#     encode = encoder(image)\n","#     name = \"unknow\"\n","#     distance = float(\"inf\")\n","#     for db_name, db_encode in encoding_dict.items():\n","#             dist = cosine(db_encode, encode)\n","#             if dist < recognition_t and dist < distance:\n","#                 name = db_name\n","#                 distance = dist\n","\n","#     return name\n","\n","# train_dataset, test_dataset = tfds.load(name=\"mnist\", split=['train', 'test'], as_supervised=True)\n","# train_dataset = train_dataset.map(format_function.format_gray_image)\n","# test_dataset = test_dataset.map(format_function.format_gray_image)\n","# combined_dataset = train_dataset.concatenate(test_dataset)\n","# combined_dataset = combined_dataset\n","\n","\n","# total = 0\n","# right_choice = 0\n","# i = 0\n","# total_image = len(combined_dataset)\n","# # plt.figure(figsize=(3*(total_image/10), 30))\n","# for image, label in combined_dataset:\n","#     predict_label = detect(image, face_net_model, load_pickle(\"encodings/encoding_train_cross_entropy.pkl\"))\n","#     print(\"predict: {} |||||| actual: {}\".format(predict_label, label))\n","#     # plt.subplot(total_image/10,10,i+1)\n","#     # plt.imshow(image.numpy())\n","#     # plt.title(\"predict: {} |||||| actual: {}\".format(predict_label, label))\n","#     # plt.axis(\"off\")\n","#     total = total + 1\n","#     if(tf.equal(predict_label,label)):\n","#         right_choice = right_choice + 1\n","#     i = i+1\n","# print(total, right_choice)"]},{"cell_type":"markdown","metadata":{"id":"ea45FkJx85vE"},"source":["# Test embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"487FrlI3IChi"},"outputs":[],"source":["# global_value = GlobalValue(image_size=[110,110], batch_size = 48, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","# format_function = FormatFunction(global_value)\n","# # Load network\n","# face_net_model = tf.keras.models.load_model(\"save_model/face_recognize_entropy4\")\n","# face_net_model = convert_train_model_to_embedding(face_net_model)\n","# #Preprocess data\n","# test_dataset = tf.data.Dataset.list_files(\"dataset/10_person/*/*\",shuffle=False)\n","# test_dataset = test_dataset.map(format_function.get_label_as_string, num_parallel_calls=tf.data.AUTOTUNE)\n","# test_dataset = test_dataset.map(format_function.process_image, num_parallel_calls=tf.data.AUTOTUNE)\n","# test_dataset = test_dataset.filter(lambda image, label: tf.math.not_equal(tf.size(image), 0))\n","# test_dataset = test_dataset.batch(20)\n","# # Evaluate the network\n","# results = face_net_model.predict(test_dataset)\n","\n","# # Save test embeddings for visualization in projector\n","# np.savetxt(\"vecs.tsv\", results, delimiter='\\t')\n","\n","# out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n","# for img, labels in tfds.as_numpy(test_dataset):\n","#     [out_m.write(str(x) + \"\\n\") for x in labels]\n","# out_m.close()\n","\n","\n","# try:\n","#   from google.colab import files\n","#   files.download('vecs.tsv')\n","#   files.download('meta.tsv')\n","# except:\n","#   pass"]},{"cell_type":"markdown","metadata":{"id":"4Y5EBEOMGJAK"},"source":["# Test embedding pytorch\n"]},{"cell_type":"markdown","metadata":{"id":"TH9iO8079PO7"},"source":["# Evaluate"]},{"cell_type":"markdown","metadata":{"id":"MEXqeyIRiyce"},"source":["## Evaluate no mask lfw"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"wNU388q-C1ii"},"outputs":[{"name":"stdout","output_type":"stream","text":["evaluate epoch:  56\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/layers/core/lambda_layer.py:305: UserWarning: train.Net is not loaded, but a Lambda layer uses it. It may cause errors.\n","  'function_type')\n","100%|**********| 5749/5749 [1:04:36<00:00,  1.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 3876.270961523056 for 11917 images, average 0.3252723807605149s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [30:28<00:00,  6.52it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9254830674164123, recall 0.9697786376111368, accuracy 0.8383821431568348, f1 0.947113220164197\n","evaluate epoch:  57\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:24<00:00,  7.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 804.305805683136 for 11917 images, average 0.06749230558723975s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:03<00:00,  6.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9219753331502101, recall 0.9664202276864791, accuracy 0.8329277502727196, f1 0.9436747574110608\n","evaluate epoch:  58\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:18<00:00,  7.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 798.4342293739319 for 11917 images, average 0.06699959967894033s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:13<00:00,  6.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9196956655027271, recall 0.9653980702922993, accuracy 0.8231937568179911, f1 0.9419928610160812\n","evaluate epoch:  59\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:27<00:00,  7.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 807.4096174240112 for 11917 images, average 0.06775275802836378s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:14<00:00,  6.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9294536751246156, recall 0.9695755653120003, accuracy 0.8455987245112024, f1 0.9490907810172227\n","evaluate epoch:  60\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:25<00:00,  7.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 805.7844679355621 for 11917 images, average 0.06761638566212655s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:12<00:00,  6.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.925359228657658, recall 0.9684676145845664, accuracy 0.8356129898464378, f1 0.9464227925691865\n","evaluate epoch:  61\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:30<00:00,  7.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 810.2897708415985 for 11917 images, average 0.06799444246384144s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:46<00:00,  6.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9158089687424289, recall 0.9643263867231664, accuracy 0.8205924309809516, f1 0.9394416749717\n","evaluate epoch:  62\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:33<00:00,  7.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 813.1376552581787 for 11917 images, average 0.06823341908686571s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:38<00:00,  6.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9270325560647591, recall 0.9698484109084095, accuracy 0.8415708651506252, f1 0.9479572698696229\n","evaluate epoch:  63\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:32<00:00,  7.08it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 812.0751354694366 for 11917 images, average 0.06814425908109731s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:25<00:00,  6.32it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9203717797406472, recall 0.9657928986530877, accuracy 0.8224385331878828, f1 0.9425354415513727\n","evaluate epoch:  64\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:25<00:00,  7.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 805.8047616481781 for 11917 images, average 0.06761808858338324s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:30<00:00,  6.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9258548697671389, recall 0.9697125386070603, accuracy 0.8351934211630444, f1 0.9472763376045201\n","evaluate epoch:  65\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:21<00:00,  7.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 801.3389418125153 for 11917 images, average 0.06724334495363894s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [31:10<00:00,  6.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9229290727432088, recall 0.9688759588370477, accuracy 0.8338508013761853, f1 0.9453445522826635\n","evaluate epoch:  66\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:27<00:00,  7.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 807.8231782913208 for 11917 images, average 0.06778746146608382s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 11917/11917 [30:47<00:00,  6.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["precision 0.9201362034188592, recall 0.9658023305681986, accuracy 0.8211798271377024, f1 0.9424163870530541\n","evaluate epoch:  67\n"]},{"name":"stderr","output_type":"stream","text":["100%|**********| 5749/5749 [13:04<00:00,  7.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["total time 784.487402677536 for 11917 images, average 0.06582926933603558s per image\n","11917\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/11917 [00:00<30:23,  6.54it/s]"]}],"source":["#Init\n","model_name = \"110-ASIAN\"\n","global_value = GlobalValue(image_size=[110,110], batch_size = 48, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","format_function = FormatFunction(global_value)\n","file_function = FileFunction()\n","\n","for i in range(56,100):\n","  print(\"evaluate epoch: \", i)\n","  path_model = os.path.join(os.getcwd(),\"save_model\",model_name,\"epoch{}\".format(i))\n","  face_net_model = tf.keras.models.load_model(path_model)\n","  face_net_model = convert_train_model_to_embedding(face_net_model)\n","  classify = Classify(face_net_model, format_function)\n","\n","  #Get embedding database\n","  encoding_path = os.path.join(os.getcwd(), \"cache\", \"encodings\", model_name, \"epoch{}.pkl\".format(i))\n","  if not os.path.exists(encoding_path):\n","    database_embedding = classify.embedding_all_data_by_directory(os.path.join(os.getcwd(),\"dataset\",\"lfw_align\"))\n","    classify.save_embedding_to_file(database_embedding, encoding_path)\n","  else:\n","    database_embedding = classify.load_embedding_from_file(encoding_path) \n","\n","  #Preprocess data\n","  data_directory = os.path.join(os.getcwd(), \"dataset\", \"lfw_align\")\n","  list_path = file_function.getPath(data_directory)\n","\n","  # Accuracy\n","  precision, recall, accuracy,f1 = classify.evaluate_using_confusion_matrix(list_path, database_embedding, 100)\n","  with open(os.path.join(os.getcwd(),\"cache\",\"metrics\",model_name+\"(no mask).csv\"), \"a\") as f:\n","    row = [i, precision, recall, accuracy, f1]\n","    writer = csv.writer(f)\n","    writer.writerow(row)\n","  print(\"precision {}, recall {}, accuracy {}, f1 {}\".format(precision, recall, accuracy,f1))"]},{"cell_type":"markdown","metadata":{"id":"MAxew-1fi6ho"},"source":["## Evaluate mask lfw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SnzVIADoirBT"},"outputs":[],"source":["#Init\n","model_name = \"110-ASIAN\"\n","global_value = GlobalValue(image_size=[110,110], batch_size = 48, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","format_function = FormatFunction(global_value)\n","file_function = FileFunction()\n","\n","for i in range(1,50):\n","  print(\"evaluate epoch: \", i)\n","  path_model = os.path.join(os.getcwd(),\"save_model\",model_name,\"epoch{}\".format(i))\n","  print(path_model)\n","  face_net_model = tf.keras.models.load_model(path_model)\n","  face_net_model = convert_train_model_to_embedding(face_net_model)\n","  classify = Classify(face_net_model, format_function)\n","\n","  #Get embedding database\n","  encoding_path = os.path.join(os.getcwd(), \"cache\", \"encodings\", model_name, \"epoch{}.pkl\".format(i))\n","  if not os.path.exists(encoding_path):\n","    database_embedding = classify.embedding_all_data_by_directory(os.path.join(os.getcwd(),\"dataset\",\"lfw_align\"))\n","    classify.save_embedding_to_file(database_embedding, encoding_path)\n","  else:\n","    database_embedding = classify.load_embedding_from_file(encoding_path)\n","\n","  #Preprocess data\n","  data_directory = os.path.join(os.getcwd(), \"dataset\", \"lfw_mask\")\n","  list_path = file_function.getPath(data_directory)\n","  \n","  # Accuracy\n","  precision, recall, accuracy,f1 = classify.evaluate_using_confusion_matrix(list_path, database_embedding, 100)\n","  with open(os.path.join(os.getcwd(),\"cache\",\"metrics\",model_name+\".csv\"), \"a\") as f:\n","    row = [i, precision, recall, accuracy, f1]\n","    writer = csv.writer(f)\n","    writer.writerow(row)"]},{"cell_type":"markdown","metadata":{"id":"gDXu9rgAo5hl"},"source":["## Evaluate no mask lfw old way"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrVPnHt8-RyT"},"outputs":[],"source":["#Init\n","global_value = GlobalValue(image_size=[110,110], batch_size = 48, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","format_function = FormatFunction(global_value)\n","file_function = FileFunction()\n","for i in range(49,50):\n","  print(\"evaluate epoch: \", i)\n","  face_net_model = tf.keras.models.load_model(\"save_model/origin/face_recognize_entropy{}\".format(i))\n","  face_net_model = convert_train_model_to_embedding(face_net_model)\n","  classify = Classify(face_net_model, format_function)\n","\n","  #Get embedding database\n","  # database_embedding = classify.embedding_all_data_by_directory_no_normalization(os.path.join(os.getcwd(),\"dataset\",\"lfw_align\"))\n","  # classify.save_embedding_to_file(database_embedding, os.path.join(os.getcwd(),\"src\",\"encodings\",\"encode_lfw_entropy{}(test)-old.pkl\".format(i)))\n","  database_embedding = classify.load_embedding_from_file(os.path.join(os.getcwd(),\"src\",\"encodings\",\"encode_lfw_entropy{}(test)-old.pkl\".format(i)))\n","  #Preprocess data\n","  data_directory = os.path.join(os.getcwd(), \"dataset\", \"lfw_align\")\n","  list_path = file_function.getPath(data_directory)\n","\n","  # Accuracy\n","  precision, recall, accuracy,f1 = classify.evaluate_using_confusion_matrix(list_path, database_embedding, 100)\n","  print(\"precision {}, recall {}, accuracy {}, f1 {}\".format(precision, recall, accuracy,f1))"]},{"cell_type":"markdown","metadata":{"id":"v1xqEHUH_EHI"},"source":["## Evaluate mask lfw old way"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ctpbVas9pDeo"},"outputs":[],"source":["#Init\n","global_value = GlobalValue(image_size=[110,110], batch_size = 48, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","format_function = FormatFunction(global_value)\n","file_function = FileFunction()\n","for i in range(49,50):\n","  print(\"evaluate epoch: \", i)\n","  face_net_model = tf.keras.models.load_model(\"save_model/origin/face_recognize_entropy{}\".format(i))\n","  face_net_model = convert_train_model_to_embedding(face_net_model)\n","  classify = Classify(face_net_model, format_function)\n","\n","  #Get embedding database\n","  # database_embedding = classify.embedding_all_data_by_directory_no_normalization(os.path.join(os.getcwd(),\"dataset\",\"lfw_align\"))\n","  # classify.save_embedding_to_file(database_embedding, os.path.join(os.getcwd(),\"src\",\"encodings\",\"encode_lfw_entropy{}(test)-old.pkl\".format(i)))\n","  database_embedding = classify.load_embedding_from_file(os.path.join(os.getcwd(),\"src\",\"encodings\",\"encode_lfw_entropy{}(test)-old.pkl\".format(i)))\n","  #Preprocess data\n","  data_directory = os.path.join(os.getcwd(), \"dataset\", \"lfw_mask\")\n","  list_path = file_function.getPath(data_directory)\n","\n","  # Accuracy\n","  precision, recall, accuracy,f1 = classify.evaluate_using_confusion_matrix(list_path, database_embedding, 100)\n","  print(\"precision {}, recall {}, accuracy {}, f1 {}\".format(precision, recall, accuracy,f1))"]},{"cell_type":"markdown","metadata":{"id":"WfTUyJoad7an"},"source":["## Compare time 128 vs 10575"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MoqNPdA-d-OP"},"outputs":[],"source":["#Init\n","global_value = GlobalValue(image_size=[110,110], batch_size = 48, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","format_function = FormatFunction(global_value)\n","file_function = FileFunction()\n","\n","for i in range(49,50):\n","  print(\"evaluate epoch: \", i)\n","  face_net_model_10575 = tf.keras.models.load_model(\"save_model/origin/face_recognize_entropy{}\".format(i))\n","  face_net_model_128 = convert_train_model_to_embedding(face_net_model_10575)\n","  #Normalize output \n","  outputs = tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(face_net_model_10575.output)\n","  face_net_model_10575 = tf.keras.Model(face_net_model_10575.input, outputs)\n","  \n","  classify_10575 = Classify(face_net_model_10575, format_function)\n","  classify_128 = Classify(face_net_model_128, format_function)\n","\n","\n","\n","  #Get embedding database\n","  database_embedding_10575 = classify_10575.embedding_all_data_by_directory(os.path.join(os.getcwd(),\"dataset\",\"lfw_align\"))\n","  database_embedding_128 = classify_128.embedding_all_data_by_directory(os.path.join(os.getcwd(),\"dataset\",\"lfw_align\"))\n","  #Preprocess data\n","  data_directory = os.path.join(os.getcwd(), \"dataset\", \"lfw_align\")\n","  list_path = file_function.getPath(data_directory)\n","  # Accuracy\n","  right_answer, unknow_answer, mis_answer, total_answer = classify_10575.evaluate(list_path, database_embedding_10575, 100)\n","  print(\"10575 accuracy {}\".format(right_answer/total_answer))\n","  right_answer, unknow_answer, mis_answer, total_answer = classify_128.evaluate(list_path, database_embedding_128, 100)\n","  print(\"128 accuracy {}\".format(right_answer/total_answer))"]},{"cell_type":"markdown","metadata":{"id":"1MfPmMpMmnqq"},"source":["## Evaluate inception resnet entropy"]},{"cell_type":"markdown","metadata":{"id":"P9K2S2ZSi9se"},"source":["# Save model weight to use locally"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpN0R7Q3tox8"},"outputs":[],"source":["face_net_model = tf.keras.models.load_model(\"save_model/face_recognize_entropy49\")\n","face_net_model.save_weights(\"model49.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OQy-AsdAE3Q"},"outputs":[],"source":["embedding = dict()\n","with open(\"database.pkl\", 'wb') as file:\n","  pickle.dump(embedding, file)"]},{"cell_type":"markdown","metadata":{"id":"agoYKchOxtpG"},"source":["# Delete corrupted image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tnipOhk_xtpG"},"outputs":[],"source":["import os\n","from tqdm import tqdm\n","from tool.FileFunction import FileFunction\n","from PIL import Image\n","import time\n","import pickle\n","\n","read_path_again = False\n","save_path = os.path.join(os.getcwd(),\"cache\",\"data\",\"path_check_corruption2.pkl\")\n","file_function = FileFunction()\n","path = os.path.join(os.getcwd(),\"face+mask_image\")\n","files = []\n","if read_path_again == True:\n","    files = file_function.getPath(path)\n","    with open(save_path, 'wb') as f:\n","        pickle.dump(files, f)\n","else:\n","    with open(save_path, 'rb') as f:\n","        files = pickle.load(f)\n","for file in tqdm(files[235984:]):\n","    try:\n","        img = Image.open(file) # open the image file\n","        img.verify() # verify that it is, in fact an image\n","    except (IOError, SyntaxError) as e:\n","        print('Bad file:', file) # print out the names of corrupt files\n","        os.remove(file)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.8.13 ('python38')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"725bcb009e1db82e62e6d35a4d1f684f39d1b4e52d6204abc611d06a7adc7e09"}}},"nbformat":4,"nbformat_minor":0}

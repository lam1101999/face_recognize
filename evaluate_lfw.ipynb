{"cells":[{"cell_type":"code","source":["# install library\n","!pip install -U tensorflow-addons\n","!pip install facenet-pytorch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bv83NsYfLtuL","executionInfo":{"status":"ok","timestamp":1677473029785,"user_tz":-420,"elapsed":9922,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"}},"outputId":"632af5a6-cccf-4d97-aa36-f808f99f6fc8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (23.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n","Installing collected packages: tensorflow-addons\n","Successfully installed tensorflow-addons-0.19.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting facenet-pytorch\n","  Downloading facenet_pytorch-2.5.2-py3-none-any.whl (1.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from facenet-pytorch) (2.25.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from facenet-pytorch) (1.22.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from facenet-pytorch) (7.1.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from facenet-pytorch) (0.14.1+cu116)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->facenet-pytorch) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->facenet-pytorch) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->facenet-pytorch) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->facenet-pytorch) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision->facenet-pytorch) (4.5.0)\n","Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision->facenet-pytorch) (1.13.1+cu116)\n","Installing collected packages: facenet-pytorch\n","Successfully installed facenet-pytorch-2.5.2\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1EbODmTUQEUD5u-1FfB_LcdTU_KWaZx3_"},"id":"foX0DvvNLT0P","executionInfo":{"status":"error","timestamp":1677485738932,"user_tz":-420,"elapsed":608717,"user":{"displayName":"Vũ Đức Lâm Nguyễn","userId":"05507541945691209538"}},"outputId":"b129119d-5a8f-4a4f-af01-89b7eb869726"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import os\n","# Mount drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","path = \"/content/drive/My Drive/Colab Notebooks/face_recognize\"\n","os.chdir(path)\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import pickle\n","import csv\n","from sklearn.datasets import fetch_lfw_pairs\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,precision_recall_fscore_support\n","from train_tensorflow.FaceNet import convert_model_to_embedding, call_instance_model\n","from train_tensorflow.Classify import Classify\n","from scipy.spatial.distance import cosine,euclidean\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","from tqdm.notebook import tqdm\n","from facenet_pytorch import MTCNN\n","from train_pytorch.detect_face import FaceDetector\n","from product.ModelController import ModelController\n","from product.FaceNetPytorch import FaceNetPytorch\n","from tool.FormatFunction import FormatFunction\n","from tool.FileFunction import FileFunction\n","from tool.GlobalValue import GlobalValue\n","\n","\n","def evaluate_lfw(model_controller, distance_type = \"euclidean\"):\n","    # setup\n","    my_model = ModelController(\"NewFacenet\")\n","    # my_model = FaceNetPytorch(\"casia-webface\")\n","    lfw = fetch_lfw_pairs(subset='test', color=True, resize=1.0, slice_=(slice(0, 250), slice(0, 250)))\n","    pairs = lfw.pairs\n","    labels = lfw.target\n","    face_detector = FaceDetector()\n","    \n","    #Loop different threshold\n","    thresholds = np.arange(0,2.01,0.01)\n","    predictions = [[] for i in range(len(thresholds))]\n","    for i in tqdm(range(0, pairs.shape[0])):\n","        pair = pairs[i]\n","        img1 = face_detector.detect_one_face(pair[0], optimize_margin = True)\n","        img2 = face_detector.detect_one_face(pair[1], optimize_margin = True)\n","        emb1 = my_model.represent(img1)\n","        prediction = my_model.verify(img1, img2, thresholds, distance_type = distance_type) #this should return 1 for same person, 0 for different persons.\n","        for j in range(len(prediction)):\n","            predictions[j].append(prediction[j])\n","    scores = []\n","    for prediction in predictions:\n","        score = accuracy_score(labels, prediction)*100\n","        scores.append(score)\n","    for j in range(len(scores)):\n","        print(f\"threshold {thresholds[j]} accuracy_score {scores[j]}\")\n","\n","def evaluate_mask_lfw():\n","    #Init\n","    model_name = \"InceptionResNetV2\"\n","    last_layer = \"ArcFace\"\n","    MODEL_NAME = f\"160-64-{model_name}-{last_layer}\"\n","    global_value = GlobalValue(image_size=[160,160], batch_size = 64, shuffle_size = 512, ratio_train = 0.8, epochs = 40, small_epochs = 2)\n","    format_function = FormatFunction(global_value)\n","    file_function = FileFunction()\n","\n","    for i in range(40,41):\n","        print(\"evaluate epoch: \", i)\n","        path_model = os.path.join(os.getcwd(),\"save_model\",\n","                                  MODEL_NAME,\"epoch{}.h5\".format(i))\n","        print(path_model)\n","        model = call_instance_model((global_value.IMAGE_SIZE[0], global_value.IMAGE_SIZE[1],3), 12593, 512, model_name, last_layer)\n","        model.load_weights(path_model)\n","        model = convert_model_to_embedding(model)\n","        model_controller = ModelController(model = model)\n","        classify = Classify(model_controller, format_function)\n","\n","        #Get embedding database, get no-mask face image then convert to vector\n","        print(\"embedding\")\n","        encoding_path = os.path.join(os.getcwd(), \"cache\", \"encodings\", model_name, \"epoch{}.pkl\".format(i))\n","        if not os.path.exists(encoding_path):\n","            database_embedding = classify.embedding_all_data_by_directory(\n","                os.path.join(os.getcwd(),\"dataset\",\"lfw_align\"))\n","            classify.save_embedding_to_file(database_embedding, encoding_path)\n","        else:\n","            database_embedding = classify.load_embedding_from_file(encoding_path)\n","\n","        #Preprocess data, get path to image with and without mask\n","        print(\"predict\")\n","        paths = list()\n","        mask_data_directory = os.path.join(os.getcwd(), \"dataset\", \"lfw_mask\")\n","        paths.extend(file_function.getPath(mask_data_directory))\n","        no_mask_data_directory = os.path.join(os.getcwd(), \"dataset\", \"lfw_align\")\n","        paths.extend(file_function.getPath(no_mask_data_directory))\n","\n","        # Accuracy\n","        actuals = [path.split(os.path.sep)[-2] for path in paths]\n","        predictions = classify.predict_list_path(paths, database_embedding, 100, distance_formula = cosine)\n","        with open(os.path.join(os.getcwd(),\"cache\",\"metrics\",model_name+\".csv\"), \"a\") as f:\n","            accuracy = accuracy_score(actuals, predictions)\n","            precision,recall, f1 = precision_recall_fscore_support(actuals, predictions,average = \"samples\")\n","            row = [i, precision, recall, accuracy, f1]\n","            writer = csv.writer(f)\n","            writer.writerow(row)\n","\n","\n","evaluate_mask_lfw()"]}],"metadata":{"kernelspec":{"display_name":"python310","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1089d176cd5639e770617b9490c2e07484c91636366b75c75a40730f35025e37"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}